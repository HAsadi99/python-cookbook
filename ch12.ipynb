{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 12. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e445e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=np.float64(7.742636826811269), max_iter=500, penalty='l1',\n",
      "                   solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# 12.1 Selecting the Best Models Using Exhaustive Search\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "\n",
    "# create logistic regression\n",
    "logistic = linear_model.LogisticRegression(max_iter=500, solver='liblinear')\n",
    "\n",
    "# create range of candidate penalty hyperparameter values\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# # create range of candidate regularization hyperparameter values\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create dictionary of hyperparameter candidates\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# create grid search\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n",
    "\n",
    "# fit grid search\n",
    "best_model = gridsearch.fit(features, target)\n",
    "\n",
    "# show the best model\n",
    "print(best_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6658a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "       5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "       3.59381366e+03, 1.00000000e+04])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(0, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3557c277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best penalty:  l1\n",
      "best C:  7.742636826811269\n"
     ]
    }
   ],
   "source": [
    "# view best hyperparameters\n",
    "print('best penalty: ', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('best C: ', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488210eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict target vector\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b782912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=np.float64(1.668088018810296), max_iter=500, penalty='l1',\n",
      "                   solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# 12.2 Selecting the Best Models Using Randomized Search\n",
    "from scipy.stats import uniform\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "\n",
    "# create logistic regression\n",
    "logistic = linear_model.LogisticRegression(max_iter=500, solver='liblinear')\n",
    "\n",
    "# Create range of candidate regularization penalty hyperparameter values\n",
    "penalty = ['l1','l2']\n",
    "\n",
    "# Create distribution of candidate regularization hyperparameter values\n",
    "C = uniform(loc=0, scale=4)\n",
    "# C =uniform(loc=0, scale=4).rvs(10)\n",
    "\n",
    "# create a hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# create randomize search\n",
    "randomizedsearch = RandomizedSearchCV(logistic, hyperparameters, random_state=1, \n",
    "                                      n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Fit randomized search\n",
    "best_model = randomizedsearch.fit(features, target)\n",
    "\n",
    "print(best_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e066bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best penalty:  l1\n",
      "best C:  1.668088018810296\n"
     ]
    }
   ],
   "source": [
    "print('best penalty: ', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('best C: ', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff56524b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scipy.stats._distn_infrastructure.rv_continuous_frozen at 0x73fa9549e290>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform(loc=0, scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9375df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.87180442, 1.81167887, 0.40267542, 2.422375  , 1.66870518,\n",
       "       1.22044736, 1.58645945, 1.57654496, 0.58810845, 3.97300242])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a uniform distribution between 0 and 4, sample 10 values\n",
    "uniform(loc=0, scale=4).rvs(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2079b99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fabbaf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('classifier',\n",
      "                 LogisticRegression(C=np.float64(7.742636826811269),\n",
      "                                    max_iter=500, penalty='l1',\n",
      "                                    solver='liblinear'))])\n"
     ]
    }
   ],
   "source": [
    "# 12.3 Selecting the Best Models from Multiple Learning Algorithms\n",
    "\n",
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# set random seed \n",
    "np.random.seed(0)\n",
    " \n",
    "iris = datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "\n",
    "# create a pipeline\n",
    "pipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n",
    "\n",
    "# Create dictionary with candidate learning algorithms and their hyperparameter\n",
    "search_space =[{\"classifier\": [LogisticRegression(max_iter=500, solver='liblinear')],\n",
    "                \"classifier__penalty\": ['l1','l2'],\n",
    "                \"classifier__C\": np.logspace(0, 4 , 10)},\n",
    "               {\"classifier\": [RandomForestClassifier()],\n",
    "                \"classifier__n_estimators\": [10, 100, 1000],\n",
    "                \"classifier__max_features\": [1, 2, 3]}]\n",
    "\n",
    "# create grid search\n",
    "gridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "\n",
    "best_model = gridsearch.fit(features, target)\n",
    "\n",
    "print(best_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f763d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=np.float64(7.742636826811269), max_iter=500, penalty='l1',\n",
      "                   solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# view best model\n",
    "print(best_model.best_estimator_.get_params()['classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9e3ac8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict target vector\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3c594ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocess',\n",
      "                 FeatureUnion(transformer_list=[('std', StandardScaler()),\n",
      "                                                ('pca', PCA(n_components=1))])),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=np.float64(7.742636826811269),\n",
      "                                    max_iter=1000, penalty='l1',\n",
      "                                    solver='liblinear'))])\n"
     ]
    }
   ],
   "source": [
    "# 12.4 Selecting the Best Models When Preprocessing\n",
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "iris =datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "\n",
    "# Create a preprocessing object that includes StandardScaler features and PCA\n",
    "preprocess =  FeatureUnion([(\"std\", StandardScaler()), (\"pca\", PCA())])\n",
    "\n",
    "# create a pipeline\n",
    "pipe = Pipeline([(\"preprocess\", preprocess),\n",
    "                 (\"classifier\", LogisticRegression(max_iter=1000, solver='liblinear'))])\n",
    "\n",
    "# Create space of candidate values\n",
    "search_space = [{\"preprocess__pca__n_components\": [1, 2, 3],\n",
    "                 \"classifier__penalty\": ['l1','l2'],\n",
    "                 \"classifier__C\": np.logspace(0, 4, 10)}]\n",
    "\n",
    "# create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# fit grid search\n",
    "best_model = clf.fit(features, target)\n",
    "\n",
    "print(best_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd4fbfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_estimator_.get_params()['preprocess__pca__n_components']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb69ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=np.float64(5.926151812475554), max_iter=500, penalty='l1',\n",
      "                   solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# 12.5 Speeding Up Model Selection with Parallelization\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    " \n",
    "# create logistic regression\n",
    "logistic = linear_model.LogisticRegression(max_iter=500, solver='liblinear')\n",
    "\n",
    "# Create range of candidate regularization penalty hyperparameter values\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "\n",
    "# Create range of candidate values for c , 1000 numbers between 10^0 to 10^4\n",
    "C = np.logspace(0, 4, 1000)\n",
    "\n",
    "# create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# create grid search\n",
    "# or we can use n_jobs = 1  to use one core of computer\n",
    "# gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs= 1, verbose=1)\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# fit grid search\n",
    "best_model = gridsearch.fit(features, target)\n",
    "\n",
    "print(best_model.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af8ec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV(Cs=100, max_iter=500, solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# 12.6 Speeding Up Model Selection Using Algorithm Specific Methods\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "\n",
    "# Create cross-validated logistic regression\n",
    "# Cs as list contains the candidate hyperparameter values to select from\n",
    "# CS as integer generates a list of that number of candidate values\n",
    "logit = linear_model.LogisticRegressionCV(Cs=100, max_iter=500, solver='liblinear')\n",
    "\n",
    "# train model \n",
    "logit.fit(features, target)\n",
    "\n",
    "print(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20124c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9733333333333334)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12.7 Evaluating Performances After Model Selection\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "\n",
    "# create a logistic regression\n",
    "logistic = linear_model.LogisticRegression(max_iter=500, solver='liblinear')\n",
    "\n",
    "# Create range of 20 candidate values for C\n",
    "C = np.logspace(0, 4, 20)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C)\n",
    "\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Conduct nested cross-validation and output the average score\n",
    "cross_val_score(gridsearch, features, target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70dd59dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=1)\n",
    "\n",
    "best_model = gridsearch.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd113b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(gridsearch, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ccb556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
